{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a4b37d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# === PATHS ===\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m BASE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m))\n\u001b[0;32m      8\u001b[0m RAW_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m PROCESSED_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# data_cleaning.py â€” FINAL & BULLETPROOF\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "\n",
    "# === PATHS ===\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "RAW_DIR = os.path.join(BASE_DIR, '..', '..', 'data', 'raw')\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, '..', '..', 'data', 'processed')\n",
    "DB_PATH = os.path.join(BASE_DIR, '..', '..', 'hsbc_risk_esg.db')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# === LOAD RAW ===\n",
    "print(\"Loading raw data...\")\n",
    "try:\n",
    "    esg_raw = pd.read_csv(os.path.join(RAW_DIR, 'SP 500 ESG Risk Ratings.csv'))\n",
    "    fin_raw = pd.read_csv(os.path.join(RAW_DIR, 'financials.csv'))\n",
    "    print(f\"ESG: {len(esg_raw)} rows | Financial: {len(fin_raw)} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "    exit()\n",
    "\n",
    "# === STANDARDIZE SYMBOLS ===\n",
    "esg_raw['Symbol'] = esg_raw['Symbol'].astype(str).str.strip().str.upper()\n",
    "fin_raw['Symbol'] = fin_raw['Symbol'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# === ESG TABLE ===\n",
    "esg_df = esg_raw[[\n",
    "    'Symbol', 'Name', 'Sector',\n",
    "    'Total ESG Risk score', 'Environment Risk Score',\n",
    "    'Social Risk Score', 'Governance Risk Score',\n",
    "    'Controversy Level', 'Controversy Score',\n",
    "    'ESG Risk Percentile', 'ESG Risk Level'\n",
    "]].copy()\n",
    "\n",
    "# === FINANCIAL TABLE ===\n",
    "financial_df = fin_raw[[\n",
    "    'Symbol', 'Name', 'Sector',\n",
    "    'Price', 'Price/Earnings', 'Dividend Yield',\n",
    "    'Earnings/Share', 'Market Cap', 'EBITDA',\n",
    "    'Price/Sales', 'Price/Book'\n",
    "]].copy()\n",
    "\n",
    "# === CLEANING ===\n",
    "print(\"Cleaning data...\")\n",
    "esg_df['Total ESG Risk score'].fillna(esg_df['Total ESG Risk score'].mean(), inplace=True)\n",
    "esg_df['ESG Risk Level'].fillna('Medium', inplace=True)\n",
    "financial_df['Price/Book'] = financial_df['Price/Book'].clip(lower=0)\n",
    "\n",
    "# === DERIVE FEATURES ===\n",
    "financial_df['debt_to_equity'] = financial_df['Price/Book'] * 1.5\n",
    "financial_df['roe'] = (financial_df['Earnings/Share'] / financial_df['Price'] * 100).fillna(0)\n",
    "\n",
    "# === RISK FLAG ===\n",
    "esg_df['risk_flag'] = esg_df['ESG Risk Level'].apply(lambda x: 1 if str(x) in ['Medium', 'High', 'Severe'] else 0)\n",
    "\n",
    "# === FINAL MERGE ===\n",
    "print(\"Merging datasets...\")\n",
    "merged_df = pd.merge(esg_df, financial_df, on='Symbol', how='inner', suffixes=('_esg', '_fin'))\n",
    "\n",
    "print(f\"After merge: {len(merged_df)} companies\")\n",
    "\n",
    "# === PRESERVE SECTOR & NAME FROM ESG ===\n",
    "merged_df['Sector'] = merged_df['Sector_esg']\n",
    "merged_df['name'] = merged_df['Name_esg']\n",
    "\n",
    "# === DROP DUPLICATES SAFELY ===\n",
    "cols_to_drop = ['Name_esg', 'Name_fin', 'Sector_esg', 'Sector_fin']\n",
    "merged_df = merged_df.drop(columns=[c for c in cols_to_drop if c in merged_df.columns])\n",
    "\n",
    "# === FINAL COLUMNS ORDER (SAFE) ===\n",
    "final_cols = [\n",
    "    'Symbol', 'name', 'Sector',\n",
    "    'Total ESG Risk score', 'Environment Risk Score',\n",
    "    'Social Risk Score', 'Governance Risk Score',\n",
    "    'Controversy Level', 'Controversy Score',\n",
    "    'ESG Risk Percentile', 'ESG Risk Level',\n",
    "    'Price', 'Price/Earnings', 'Dividend Yield',\n",
    "    'Earnings/Share', 'Market Cap', 'EBITDA',\n",
    "    'Price/Sales', 'Price/Book',\n",
    "    'debt_to_equity', 'roe', 'risk_flag'\n",
    "]\n",
    "\n",
    "# Only keep columns that exist\n",
    "available_cols = [c for c in final_cols if c in merged_df.columns]\n",
    "missing_cols = [c for c in final_cols if c not in merged_df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"Warning: Missing columns: {missing_cols}\")\n",
    "    for col in missing_cols:\n",
    "        merged_df[col] = None  # or 0\n",
    "\n",
    "merged_df = merged_df[available_cols]\n",
    "\n",
    "print(f\"Final dataset: {len(merged_df)} companies | {len(merged_df.columns)} columns\")\n",
    "\n",
    "# === SAVE TO CSV ===\n",
    "csv_path = os.path.join(PROCESSED_DIR, 'merged_data.csv')\n",
    "merged_df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved to {csv_path}\")\n",
    "\n",
    "# === SAVE TO SQL ===\n",
    "try:\n",
    "    engine = create_engine(f'sqlite:///{DB_PATH}')\n",
    "    esg_df.to_sql('esg_data', engine, if_exists='replace', index=False)\n",
    "    financial_df.to_sql('financial_data', engine, if_exists='replace', index=False)\n",
    "    risk_labels = esg_df[['Symbol', 'Name', 'risk_flag']].copy()\n",
    "    risk_labels['compliance_score'] = esg_df['Controversy Score'].fillna(1)\n",
    "    risk_labels.to_sql('risk_labels', engine, if_exists='replace', index=False)\n",
    "    print(\"SQL tables updated.\")\n",
    "except Exception as e:\n",
    "    print(f\"SQL Error: {e}\")\n",
    "\n",
    "print(\"data_cleaning.py COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
